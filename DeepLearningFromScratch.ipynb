{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8899d0d1-8f45-488a-9e6b-f83ec350e741",
   "metadata": {},
   "source": [
    "# **Deep learning neural network from scratch**\n",
    "By Gabriel PiÅ¡vejc (gabrielzelva@gmail.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f23aff-12ab-47f3-bcbb-0b3105d3ebd4",
   "metadata": {},
   "source": [
    "In this notebook, I will create a deep neural network from scratch using linear algebra and calculus. The network will then be used to tell genuine bank notes from forged ones.\n",
    "\n",
    "The dataset I will be using can be found here:\n",
    "\n",
    "Lohweg, V. (2012). Banknote Authentication [Dataset]. UCI Machine Learning Repository. https://doi.org/10.24432/C55P57."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9498708-946d-412f-8eca-be7dc9af7e0f",
   "metadata": {},
   "source": [
    "## **Information about the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad787d95-f7cf-4ad7-aa63-ebc902cec717",
   "metadata": {},
   "source": [
    "The original author provides a good description of what the data actually is: \n",
    "\n",
    "> Data were extracted from images that were taken from genuine and forged banknote-like specimens.  For digitization, an industrial camera usually used for print inspection was used. The final images have 400x 400 pixels. Due to the object lens and distance to the investigated object gray-scale pictures with a resolution of about 660 dpi were gained. Wavelet Transform tool were used to extract features from images.\n",
    "\n",
    "Additionally, I should mention that the data contains 4 predictors: \n",
    "- Variance\n",
    "- Skewness\n",
    "- Curtosis\n",
    "- Entropy\n",
    "\n",
    "The target is of course a boolean variable which indicates whether a certain specimen is genuine or forged. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1120a2cc-b53b-4c17-9ac3-00b4804d78c8",
   "metadata": {},
   "source": [
    "Finally, since the dataset is not particularly large, I will be working with a seed to prevent bad luck ruining this demonstration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92a58967-8ff2-4e4d-99d4-040d28d5a87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 3091488470"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edeb912c-6d44-43c4-a7ea-e9ba81df187f",
   "metadata": {},
   "source": [
    "# **Creating the neural network**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4eb9e8c-e987-4113-be75-28e2d187d13a",
   "metadata": {},
   "source": [
    "In this section, I will show the step by step process for creating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5e34da-c5fa-4aba-ba7e-79170e802558",
   "metadata": {},
   "source": [
    "## **Data preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526a0726-f1b4-4d2f-bcbf-12b800789fa6",
   "metadata": {},
   "source": [
    "First things first, beside actually loading the data, we will also need to preprocess it. That means:\n",
    "- Shuffle it in order to prevent any ordering bias\n",
    "- Map the predictors to a standard normal distribution\n",
    "- Separate the predictors from the response\n",
    "- Separate the training data from the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89d3cc49-4336-45e0-a3db-09b0d69162c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"data_banknote_authentication.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b5976aa-6028-4b69-8e3a-da069fcb6f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling the data\n",
    "\n",
    "data = data.sample(frac = 1,\n",
    "                   random_state = seed,\n",
    "                   replace = False,\n",
    "                   ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08c62d1a-7670-4d4b-abe4-81471ac03b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping the predictors to a standard normal distribution\n",
    "\n",
    "data.iloc[:, :4] = data.iloc[:, :4].apply(\n",
    "    lambda col: (col - col.mean()) / col.std()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7357b094-8a99-429f-95cb-2826f425b707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating the predictors from the response\n",
    "\n",
    "predictors = data.iloc[:, :4]\n",
    "\n",
    "response = data.iloc[:, 4:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2aacf38b-8217-40b7-b0c5-d50ce7173715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating the testing data from the training data\n",
    "import numpy as np \n",
    "\n",
    "split_index = int(len(data) * 2/3)\n",
    "\n",
    "training_predictors = predictors.iloc[:split_index, :]\n",
    "training_predictors = np.array(training_predictors)\n",
    "\n",
    "testing_predictors = predictors.iloc[split_index:, ]\n",
    "testing_predictors = np.array(testing_predictors)\n",
    "\n",
    "\n",
    "training_response = response.iloc[:split_index, :]\n",
    "training_response = np.array(training_response)\n",
    "\n",
    "testing_response = response.iloc[split_index:, :]\n",
    "testing_response = np.array(testing_response)\n",
    "\n",
    "\n",
    "\n",
    "del predictors, response, split_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e58086c-dbad-4be8-8f6b-f6cff0cc4970",
   "metadata": {},
   "source": [
    "## **Declaring the neural network**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4fe1b4-b776-413a-91c3-62105425c934",
   "metadata": {},
   "source": [
    "I want to create a neural network with the following characteristics: \n",
    "- 4 input nodes, one per each predictor\n",
    "- 2 hidden layers, each containing 16 nodes and followed by the ReLU activation function\n",
    "- 1 output node, followed by the logistic function to predict a binary outcome (the bank note is either forged or genuine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4567ae70-5079-45bd-8bf5-6f908e85853d",
   "metadata": {},
   "source": [
    "We can see the desired network on the following diagram: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39500ca5-165c-453e-9620-34c28b841e84",
   "metadata": {},
   "source": [
    "<img src=\"Diagram.jpeg\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7615f5f6-7c63-40f9-8886-54b994710515",
   "metadata": {},
   "source": [
    "### **The maths behind the structure**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a072e86-4b92-4b47-9c5d-4a18f0714b09",
   "metadata": {},
   "source": [
    "#### **The tensors**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb5dd1c-b0d6-4f69-9507-66b03a83082e",
   "metadata": {},
   "source": [
    "To build this neural network, we will need to view each hidden layer as a matrix of weights and a vector of biases. Therefore, the first hidden layer can be expressed like so:\n",
    "\n",
    "$\n",
    "W_{FirstHidden} \\begin{bmatrix}\n",
    "w_{1.1} & w_{1.2} & \\cdots & w_{1.16} \\\\\n",
    "w_{2.1} & w_{2.2} & \\cdots & w_{2.16} \\\\\n",
    "w_{3.1} & w_{3.2} & \\cdots & w_{3.16} \\\\\n",
    "w_{4.1} & w_{4.2} & \\cdots & w_{4.16}\n",
    "\\end{bmatrix}\n",
    "$\n",
    "$\n",
    "B_{FirstHidden} = \\begin{bmatrix}\n",
    "b_1 \\\\\n",
    "b_2 \\\\\n",
    "\\vdots \\\\\n",
    "b_{16}\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "The second hidden layer like so: \n",
    "\n",
    "$\n",
    "W_{SecondHidden} \\begin{bmatrix}\n",
    "w_{1.1} & w_{1.2} & \\cdots & w_{1.16} \\\\\n",
    "w_{2.1} & w_{2.2} & \\cdots & w_{2.16} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w_{16.1} & w_{16.2} & \\cdots & w_{16.16}\n",
    "\\end{bmatrix}\n",
    "$\n",
    "$\n",
    "B_{SecondHidden} = \\begin{bmatrix}\n",
    "b_1 \\\\\n",
    "b_2 \\\\\n",
    "\\vdots \\\\\n",
    "b_{16}\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02397877-8bf6-4a9f-bc1f-3593290ef9bc",
   "metadata": {},
   "source": [
    "And finally, the output layer can be expressed like a simple vector of the 16 weights converging to a single node and the remaining bias can be seen as a scalar. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6e0a77-240c-4c1c-8431-a10daf089183",
   "metadata": {},
   "source": [
    "$\n",
    "W_{Output} = \\begin{bmatrix}\n",
    "w_1 \\\\\n",
    "w_2 \\\\\n",
    "\\vdots \\\\\n",
    "w_{16}\n",
    "\\end{bmatrix}\n",
    "$\n",
    "$\n",
    "B_{Output} = [b]\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb0c913-b4fc-4172-b8dd-0453da30c6ca",
   "metadata": {},
   "source": [
    "Of course, we will define all of these tensors in Python using NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a8fb5f6-0189-4281-ae72-f5dfe2f3542a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We shouldn't really define it just now, but for narrative purposes, I will leave the block here.\n",
    "# However, we will need to redefine it later\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "w_first_hidden = np.random.rand(16, 4)\n",
    "b_first_hidden = np.random.rand(16, 1)\n",
    "\n",
    "w_second_hidden = np.random.rand(16, 16)\n",
    "b_second_hidden = np.random.rand(16, 1)\n",
    "\n",
    "w_output = np.random.rand(1, 16)\n",
    "b_output = np.random.rand(1, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c1f3cb-275f-4ebd-aa1d-9950e4e9c9d2",
   "metadata": {},
   "source": [
    "#### **Activation functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99954cc-50a0-45b1-8a56-7d33988919c3",
   "metadata": {},
   "source": [
    "Needless to say, we also need the activation functions. As we have previously established, we will use ReLU for the two hidden layers and the logistic function for the output. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529b4489-e505-4d88-8a15-f7523a4e22bb",
   "metadata": {},
   "source": [
    "$ \\Large\n",
    "ReLU(x) = \\frac{x + |x|}{2}\n",
    "$\n",
    "$\\Large\n",
    "Logistic(x) = \\frac{1}{1+e^{-x}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f453e1a8-b726-4a83-b3c6-b161f7f6a53a",
   "metadata": {},
   "source": [
    "We can define them in Python like so: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4df5c08b-5490-4a9e-99dd-3cd9d456d811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x): \n",
    "    return np.maximum(x, 0) \n",
    "    # We could use the function literally: \n",
    "    # return (x + np.abs(x)) / 2\n",
    "    # But np.maximum does exactly the same thing while being much more readeable, so, I am going with that. \n",
    "    \n",
    "def logistic(x): \n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74f3998-255c-458c-b7b7-f08c490d9dce",
   "metadata": {},
   "source": [
    "## **Making the structure work**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4257cda0-ddff-4169-8c33-66de680f7dd7",
   "metadata": {},
   "source": [
    "Now that we have created the individual blocks of our neural network, we also need to force them to operate like one. That is, we need to:\n",
    "\n",
    "- Create a function for the forward propagation which will allow us to make predictions.\n",
    "- Create a function for the backward propagation which will allow us to actually train the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26b34cf-d2ba-47a6-a16f-583f9737560d",
   "metadata": {},
   "source": [
    "### **Forward propagation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d257f82-cf05-4e19-a3f4-45f92f1d7d52",
   "metadata": {},
   "source": [
    "For a prediction to be made, the following needs to happen: \n",
    "1. The values of the 4 predictors need to be passed into the first hidden layer as a vector.\n",
    "2. This will cause a matrix-vector multiplication between the entry vector and the weight matrix.\n",
    "3. The layer will also sum the vector of biases to the now multiplied vector.\n",
    "4. ReLU will be applied to the resulting vector.\n",
    "5. Having finished with the first hidden layer, the new vector will be passed to the second one which will repeat the steps 2, 3 and 4, however, using its own weight matrix and bias vector.\n",
    "6. Once the data is out of the hidden layers, it will converge to the output node. This will force the hidden layers result to be multiplied with the output weigh vector, producing a single scalar result.\n",
    "7. The output bias will be summed to it.\n",
    "8. The logistic function will be applied, mapping the result between 0 and 1. This is our result, since rounding this number will give us a prediction to whether a particular banknote was forged or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4076d627-aefc-43c4-a71c-4af315bda2ce",
   "metadata": {},
   "source": [
    "Of course, we will do all of this via a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a705a47-0bad-4e92-9126-1fa4de82d7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(input_vector):\n",
    "    \n",
    "    first_algebra = w_first_hidden @ input_vector + b_first_hidden\n",
    "    first_function = relu(first_algebra)\n",
    "    \n",
    "    second_algebra = w_second_hidden @ first_function + b_second_hidden\n",
    "    second_function = relu(second_algebra)\n",
    "    \n",
    "    output_algebra = w_output @ second_function + b_output\n",
    "    result = logistic(output_algebra)\n",
    "    \n",
    "    return first_algebra, first_function, second_algebra, second_function, output_algebra, result\n",
    "    # We will also save the temporary states of the process for future use during training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c359c63f-cbf6-4d8c-b5a0-cc56eb3d38ca",
   "metadata": {},
   "source": [
    "### **Backward propagation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e18b380-04c4-47d5-a48d-3b8eb1a334fc",
   "metadata": {},
   "source": [
    "Machine learning usually boils down to optimizing a loss function. However, unlike with linear regression, it can be a bit more tricky to take the derivative with respect to a certain variable, since, as we might suspect from seeing the forward propagation function, the loss function features some deep nesting. However, we can untangle this using the chain rule. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c491098a-1dd2-407a-8601-f6610087f9ea",
   "metadata": {},
   "source": [
    "We will look at how it is done in a moment, however, we first need to define our loss function. In this particular case, we will use the square error. I should also mention that in order to make the connection to the forward propagation easier, I will keep using the variable names from the function defined above. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867a2d4e-3ad9-41b2-bcaf-964c5e10f0a2",
   "metadata": {},
   "source": [
    "$ \\Large Loss = (result - real\\_value)^2 $ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ddb779-0dec-4734-97cf-024c73cb5223",
   "metadata": {},
   "source": [
    "This formula, while being technically correct, is deceptively simple. The  reality is that \"result\" is actually nesting a series of functions that look like so: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154a3c64-3e95-4499-8d8b-2acbf44ef6dc",
   "metadata": {},
   "source": [
    "$\n",
    "result = Logistic(output\\_algebra)\n",
    "$\n",
    "\n",
    "$\n",
    "output\\_algebra = w\\_output \\cdot second\\_function + b\\_output\n",
    "$\n",
    "\n",
    "$\n",
    "second\\_function = ReLU(second\\_algebra)\n",
    "$\n",
    "\n",
    "$\n",
    "second\\_algebra = w\\_second\\_hidden \\cdot first\\_function + b\\_second\\_hidden\n",
    "$\n",
    "\n",
    "$\n",
    "first\\_function = ReLU(first\\_algebra)\n",
    "$\n",
    "\n",
    "$\n",
    "first\\_algebra = w\\_first\\_hidden \\cdot input\\_vector + b\\_first\\_hidden\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2ac320-eba9-4bd3-9648-6572d89d5704",
   "metadata": {},
   "source": [
    "Knowing this, we can use the chain rule to get all the derivatives we need. For example, if we were curious about the impact of the weights converging towards the output node, we could find their slope like so: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28a0eca-f1aa-416b-b921-6b346b9462e6",
   "metadata": {},
   "source": [
    "$\\Large\n",
    "\\frac{\\delta \\; loss}{\\delta \\; result} = 2 \\cdot result - 2 \\cdot real\\_value\n",
    "$\n",
    "\n",
    "$\\Large\n",
    "\\frac{\\delta \\; result}{\\delta \\; output\\_algebra} = \\frac{e^{-output\\_algebra}}{(1+e^{-output\\_algebra})^2}\n",
    "$\n",
    "\n",
    "$\\Large\n",
    "\\frac{\\delta \\; output\\_algebra}{\\delta \\; w\\_output} = second\\_function\n",
    "$\n",
    "\n",
    "$\\Large\n",
    "\\therefore\n",
    "$\n",
    "\n",
    "$\\Large\n",
    "\\frac{\\delta \\; loss}{\\delta \\; w\\_output} = \n",
    "\\frac{\\delta \\; loss}{\\delta \\; result} \\cdot\n",
    "\\frac{\\delta \\; result}{\\delta \\; output\\_algebra}\\cdot\n",
    "\\frac{\\delta \\; output\\_algebra}{\\delta \\; w\\_output} = \n",
    "$\n",
    "\n",
    "$\\Large\n",
    "(2 \\cdot result - 2 \\cdot real\\_value) \\cdot\n",
    "(\\frac{e^{-output\\_algebra}}{(1+e^{-output\\_algebra})^2}) \\cdot\n",
    "second\\_function\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e87535d-5142-456f-b7ff-d9772cbe066e",
   "metadata": {},
   "source": [
    "While we obviously want to optimise each and every parameter inside our network, you might have noticed that the maths get really chaotic really quickly when done by hand. Therefore, we will find the derivatives we need using Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d303fb-d52b-4a61-a081-22a24691fbcd",
   "metadata": {},
   "source": [
    "First things first, we will find all of the individual derivatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9b6f78c-7323-4cfd-a42e-0bbb26a87bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_loss__d_result =  -2*real_value + 2*result\n",
      "d_result__d_output_algebra =  exp(-output_algebra)/(1 + exp(-output_algebra))**2\n",
      "d_output_algebra__d_w_output =  second_function\n",
      "d_output_algebra__d_b_output =  1\n",
      "d_output_algebra__d_second_function =  w_output\n",
      "d_second_function__d_second_algebra =  Heaviside(second_algebra)\n",
      "d_second_algebra__d_w_second_hidden =  first_function\n",
      "d_second_algebra__d_b_second_hidden =  1\n",
      "d_second_algebra__d_first_function =  w_second_hidden\n",
      "d_first_function__d_first_algebra =  Heaviside(first_algebra)\n",
      "d_first_algebra__d_w_first_hidden =  input_vector\n",
      "d_first_algebra__d_b_first_hidden =  1\n"
     ]
    }
   ],
   "source": [
    "import sympy as sp\n",
    "\n",
    "# sympy doesnt't work with numpy's functions, therefore so for the defivatives purpuse,\n",
    "# I will create a sympy version of ReLU and the logistic function\n",
    "\n",
    "def sp_relu(x): \n",
    "    return sp.Max(x, 0)\n",
    "\n",
    "def sp_logistic(x): \n",
    "    return 1 / (1 + sp.exp(-x)) \n",
    "\n",
    "# Define all the functions and take their derivatives\n",
    "\n",
    "result, real_value, output_algebra, w_output, second_function, b_output, second_algebra, w_second_hidden, first_function, b_second_hidden, first_algebra, w_first_hidden, input_vector, b_first_hidden = \\\n",
    "    sp.symbols('result real_value output_algebra w_output second_function b_output second_algebra w_second_hidden first_function b_second_hidden first_algebra w_first_hidden input_vector b_first_hidden')\n",
    "\n",
    "## Loss function\n",
    "loss = (result - real_value)**2\n",
    "\n",
    "d_loss__d_result = sp.diff(loss, result)\n",
    "print(\"d_loss__d_result = \", d_loss__d_result)\n",
    "\n",
    "## Output layer\n",
    "result = sp_logistic(output_algebra)\n",
    "\n",
    "d_result__d_output_algebra = sp.diff(result, output_algebra)\n",
    "print(\"d_result__d_output_algebra = \", d_result__d_output_algebra)\n",
    "\n",
    "output_algebra = w_output * second_function + b_output\n",
    "\n",
    "d_output_algebra__d_w_output = sp.diff(output_algebra, w_output)\n",
    "print(\"d_output_algebra__d_w_output = \", d_output_algebra__d_w_output)\n",
    "\n",
    "d_output_algebra__d_b_output = sp.diff(output_algebra, b_output)\n",
    "print(\"d_output_algebra__d_b_output = \", d_output_algebra__d_b_output)\n",
    "\n",
    "d_output_algebra__d_second_function = sp.diff(output_algebra, second_function)\n",
    "print(\"d_output_algebra__d_second_function = \", d_output_algebra__d_second_function)\n",
    "\n",
    "## Second hidden layer\n",
    "second_function = sp_relu(second_algebra)\n",
    "\n",
    "d_second_function__d_second_algebra = sp.diff(second_function, second_algebra)\n",
    "print(\"d_second_function__d_second_algebra = \", d_second_function__d_second_algebra)\n",
    "\n",
    "second_algebra = w_second_hidden * first_function + b_second_hidden\n",
    "\n",
    "d_second_algebra__d_w_second_hidden = sp.diff(second_algebra, w_second_hidden)\n",
    "print(\"d_second_algebra__d_w_second_hidden = \", d_second_algebra__d_w_second_hidden)\n",
    "\n",
    "d_second_algebra__d_b_second_hidden = sp.diff(second_algebra, b_second_hidden)\n",
    "print(\"d_second_algebra__d_b_second_hidden = \", d_second_algebra__d_b_second_hidden)\n",
    "\n",
    "d_second_algebra__d_first_function = sp.diff(second_algebra, first_function)\n",
    "print(\"d_second_algebra__d_first_function = \", d_second_algebra__d_first_function)\n",
    "\n",
    "## First hidden layer\n",
    "first_function = sp_relu(first_algebra)\n",
    "\n",
    "d_first_function__d_first_algebra = sp.diff(first_function, first_algebra)\n",
    "print(\"d_first_function__d_first_algebra = \", d_first_function__d_first_algebra)\n",
    "\n",
    "first_algebra = w_first_hidden * input_vector + b_first_hidden\n",
    "\n",
    "d_first_algebra__d_w_first_hidden = sp.diff(first_algebra, w_first_hidden)\n",
    "print(\"d_first_algebra__d_w_first_hidden = \", d_first_algebra__d_w_first_hidden)\n",
    "\n",
    "d_first_algebra__d_b_first_hidden = sp.diff(first_algebra, b_first_hidden)\n",
    "print(\"d_first_algebra__d_b_first_hidden = \", d_first_algebra__d_b_first_hidden)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3214ce9-a797-48d3-b2ee-dc543965f6b4",
   "metadata": {},
   "source": [
    "We can use these and implement the chain rule as a function which will find the gradients of all of the matrices and vectors that can be used to optimise our network. This is what is called Backward propagation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa471435-ecac-4522-a6b4-2dbf026ea685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we continue, I will just redefine the linear algebra from before. The last code block messes it up. \n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "w_first_hidden = np.random.rand(16, 4)\n",
    "b_first_hidden = np.random.rand(16, 1)\n",
    "\n",
    "w_second_hidden = np.random.rand(16, 16)\n",
    "b_second_hidden = np.random.rand(16, 1)\n",
    "\n",
    "w_output = np.random.rand(1, 16)\n",
    "b_output = np.random.rand(1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37c3b322-120e-4310-8fd7-31c0f94be9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_relu(x):\n",
    "    return x > 0\n",
    "\n",
    "def d_logistic(x):\n",
    "    return np.exp(-x) / (1 + np.exp(-x)) ** 2\n",
    "\n",
    "def backward_propagation(first_algebra, first_function, second_algebra, second_function, output_algebra, result, input_vector, real_value):\n",
    "    # Output layer\n",
    "    d_loss__d_result = 2 * (result - real_value)  # (1, 1)\n",
    "    d_result__d_output_algebra = d_logistic(output_algebra)  # (1, 1)\n",
    "    d_loss__d_output_algebra = d_loss__d_result * d_result__d_output_algebra  # (1, 1)\n",
    "\n",
    "    d_loss__d_w_output = d_loss__d_output_algebra @ second_function.T  # (1, 16)\n",
    "    d_loss__d_b_output = d_loss__d_output_algebra  # (1, 1)\n",
    "\n",
    "    d_loss__d_second_function = w_output.T @ d_loss__d_output_algebra  # (16, 1)\n",
    "\n",
    "    # Second hidden layer\n",
    "    d_second_function__d_second_algebra = d_relu(second_algebra)  # (16, 1)\n",
    "    d_loss__d_second_algebra = d_second_function__d_second_algebra * d_loss__d_second_function  # (16, 1)\n",
    "\n",
    "    d_loss__d_w_second_hidden = d_loss__d_second_algebra @ first_function.T  # (16, 16)\n",
    "    d_loss__d_b_second_hidden = d_loss__d_second_algebra  # (16, 1)\n",
    "\n",
    "    # First hidden layer\n",
    "    d_loss__d_first_function = w_second_hidden.T @ d_loss__d_second_algebra  # (16, 1)\n",
    "    d_first_function__d_first_algebra = d_relu(first_algebra)  # (16, 1)\n",
    "    d_loss__d_first_algebra = d_first_function__d_first_algebra * d_loss__d_first_function  # (16, 1)\n",
    "\n",
    "    d_loss__d_w_first_hidden = d_loss__d_first_algebra @ input_vector.T  # (16, 4)\n",
    "    d_loss__d_b_first_hidden = d_loss__d_first_algebra  # (16, 1)\n",
    "\n",
    "    return (d_loss__d_w_output, d_loss__d_b_output,\n",
    "            d_loss__d_w_second_hidden, d_loss__d_b_second_hidden,\n",
    "            d_loss__d_w_first_hidden, d_loss__d_b_first_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825457c5-bdb8-42eb-a51a-4d81e086d57d",
   "metadata": {},
   "source": [
    "## **Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0431908-4cac-4ef1-8eee-02afd077fb45",
   "metadata": {},
   "source": [
    "Now that we have defined both the forward and backward propagation, we can start training the network using stochastic gradient descent (SGT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62346071-fd6d-408a-bd88-d0ef84b61075",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "learning_rate = .01\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(len(training_predictors)):\n",
    "        \n",
    "        # get a batch \n",
    "        predictor_batch = training_predictors[i].reshape(-1, 1)\n",
    "        target_batch = training_response[i]\n",
    "\n",
    "        # Run the forward propagation\n",
    "        first_algebra, first_function, second_algebra, second_function, output_algebra, result = forward_propagation(predictor_batch)\n",
    "\n",
    "        # Run the backward propagation\n",
    "        d_loss__d_w_output, d_loss__d_b_output, d_loss__d_w_second_hidden, d_loss__d_b_second_hidden, d_loss__d_w_first_hidden, d_loss__d_b_first_hidden =  backward_propagation(first_algebra, first_function, second_algebra, second_function, output_algebra, result, predictor_batch, target_batch)\n",
    "        \n",
    "        # update weights and biases\n",
    "        w_first_hidden -= learning_rate * d_loss__d_w_first_hidden\n",
    "        b_first_hidden -= learning_rate * d_loss__d_b_first_hidden\n",
    "        \n",
    "        w_second_hidden -= learning_rate * d_loss__d_w_second_hidden\n",
    "        b_second_hidden -= learning_rate * d_loss__d_b_second_hidden\n",
    "        \n",
    "        w_output -= learning_rate * d_loss__d_w_output\n",
    "        b_output -= learning_rate * d_loss__d_b_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903e9ef9-fc6c-40af-8b76-6c30e478b4bd",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c1327d-84b3-43ad-b24b-a607c62e8957",
   "metadata": {},
   "source": [
    "Finally, as the weights and biases have been optimised, we can test how well does the model perform on data it hasn't seen before: The testing split we made at the start. We can get the confusion matrix and the precision like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bc06e28-e46f-4cca-95dd-b24d29ed4c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "          Predicted 0    Predicted 1\n",
      "Actual 0     266             0\n",
      "Actual 1     1             190\n",
      "\n",
      "Accuracy:  99.78118161925602 %\n"
     ]
    }
   ],
   "source": [
    "predictions = forward_propagation(testing_predictors.T)[5].round().astype(int)\n",
    "\n",
    "TP = FP = TN = FN = 0\n",
    "\n",
    "for actual, predicted in zip(testing_response.flatten().tolist(), predictions.flatten().tolist()):\n",
    "    if actual == 1 and predicted == 1:\n",
    "        TP += 1\n",
    "    elif actual == 0 and predicted == 0:\n",
    "        TN += 1\n",
    "    elif actual == 0 and predicted == 1:\n",
    "        FP += 1\n",
    "    elif actual == 1 and predicted == 0:\n",
    "        FN += 1\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(f\"          Predicted 0    Predicted 1\")\n",
    "print(f\"Actual 0     {TN}             {FP}\")\n",
    "print(f\"Actual 1     {FN}             {TP}\")\n",
    "print(\"\")\n",
    "\n",
    "accuracy = (predictions == testing_response.T).mean() \n",
    "print(\"Accuracy: \", accuracy * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30fc752-f70e-4eb2-b463-066fe9c8720a",
   "metadata": {},
   "source": [
    "That is, the model correctly analysed nearly all of the bank notes, except for one. However, this should be taken with a grain of salt.\n",
    "\n",
    "# **UNDER ABSOLUTELY NO CIRCUMSTANCES SHOULD THIS MODEL BE USED IN PRACTICE**. \n",
    "\n",
    "Speaking a bit more directly, **this is just a cool maths demonstration using a random dataset I found online, NOT an actual program to detect forgery**. \n",
    "\n",
    "- There are way better ways to create neural networks. (Like Torch or TensorFlow...)\n",
    "- I did not independently verify the data collection and processing methodology.\n",
    "- What currency are even the bank notes in the data? I don't think the publication actually says that. \n",
    "- The dataset is quite small.\n",
    "- A plethora of things are likely to go wrong if applied to real world problems.\n",
    "\n",
    "**So please, don't actually use the model out there. It was never meant for the real world.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
